# VERSION 1.10.3
# DESCRIPTION: Airflow container with JAVA 8 and SPARK binaries
# BUILD: docker build --rm -t docker-airflow-spark:latest .
# BASED ON: https://github.com/puckel/docker-airflow

# stretch debian version allows JAVA JDK 8
FROM python:3.9-buster

# Never prompts the user for choices on installation/configuration of packages
ENV DEBIAN_FRONTEND noninteractive
ENV TERM linux

# Airflow
ARG AIRFLOW_VERSION=1.10.7
ARG AIRFLOW_HOME=/usr/local/airflow
ARG AIRFLOW_DEPS=""
ARG PYTHON_DEPS=""
ARG SPARK_VERSION="3.1.2"
ARG HADOOP_VERSION="3.2"
ENV AIRFLOW_GPL_UNIDECODE yes

# Define Locale to de_DE
ENV LANGUAGE=de_DE.UTF-8
ENV LANG=de_DE.UTF-8
ENV LC_ALL=de_DE.UTF-8
ENV LC_CTYPE=de_DE.UTF-8
ENV LC_MESSAGES=de_DE.UTF-8

COPY requirements.txt /requirements.txt

RUN set -ex \
    && buildDeps=' \
        freetds-dev \
        libkrb5-dev \
        libsasl2-dev \
        libssl-dev \
        libffi-dev \
        libpq-dev \
        git \
    ' \
    && apt-get update -yqq \
    && apt-get install -yqq --no-install-recommends \
        $buildDeps \
        freetds-bin \
        build-essential \
        default-libmysqlclient-dev \
        apt-utils \
        curl \
        rsync \
        netcat \
        locales \
        iputils-ping \
        telnet \
    && echo 'de_DE.UTF-8 UTF-8' > /etc/locale.gen \
    && locale-gen \
    && update-locale LANG=de_DE.UTF-8 LC_ALL=de_DE.UTF-8 \
    # Ensure locales are properly set
    && dpkg-reconfigure --frontend=noninteractive locales \
    && useradd -ms /bin/bash -d ${AIRFLOW_HOME} airflow \
    && pip install -U pip setuptools wheel \
    && pip install pytz pyOpenSSL ndg-httpsclient pyasn1 \
    && pip install "pip<24.1" \
    && pip install -r requirements.txt \
    && pip install apache-airflow[crypto,celery,postgres,hive,jdbc,mysql,ssh${AIRFLOW_DEPS:+,}${AIRFLOW_DEPS}]==${AIRFLOW_VERSION} \
    && pip install 'redis>=2.10.5,<3' \
    && if [ -n "${PYTHON_DEPS}" ]; then pip install ${PYTHON_DEPS}; fi \
    && apt-get purge --auto-remove -yqq $buildDeps \
    && apt-get autoremove -yqq --purge \
    && apt-get clean \
    && rm -rf /var/lib/apt/lists/* /tmp/* /var/tmp/* /usr/share/man /usr/share/doc /usr/share/doc-base

RUN python --version && pip freeze

###############################
# Begin JAVA installation
###############################
RUN set -ex \
    && apt-get update && apt-get install -y software-properties-common gnupg2 \
    && apt-key adv --keyserver keyserver.ubuntu.com --recv-keys EB9B1D8886F44E2A \
    && add-apt-repository "deb http://security.debian.org/debian-security stretch/updates main" \
    && apt-get update \
    && apt-get install -y openjdk-8-jdk \
    && java -version \
    && javac -version
    
ENV JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64

###############################
# Finish JAVA installation
###############################

COPY entrypoint.sh /entrypoint.sh
COPY config/airflow.cfg ${AIRFLOW_HOME}/airflow/airflow.cfg

###############################
# SPARK files and variables
###############################
ENV SPARK_HOME=/usr/local/spark

RUN set -ex \
    && cd /tmp \
    && wget --no-verbose https://archive.apache.org/dist/spark/spark-${SPARK_VERSION}/spark-${SPARK_VERSION}-bin-hadoop${HADOOP_VERSION}.tgz \
    && tar -xvzf spark-${SPARK_VERSION}-bin-hadoop${HADOOP_VERSION}.tgz \
    && mkdir -p ${SPARK_HOME}/bin ${SPARK_HOME}/assembly/target/scala-2.12/jars \
    && cp -a spark-${SPARK_VERSION}-bin-hadoop${HADOOP_VERSION}/bin/. ${SPARK_HOME}/bin/ \
    && cp -a spark-${SPARK_VERSION}-bin-hadoop${HADOOP_VERSION}/jars/. ${SPARK_HOME}/assembly/target/scala-2.12/jars/ \
    && rm spark-${SPARK_VERSION}-bin-hadoop${HADOOP_VERSION}.tgz

ENV PATH=${PATH}:${SPARK_HOME}/bin

###############################
# Finish SPARK files and variables
###############################

RUN chown -R airflow: ${AIRFLOW_HOME} \
    && chmod +x /entrypoint.sh

EXPOSE 8080 5555 8793

USER airflow
WORKDIR ${AIRFLOW_HOME}
ENTRYPOINT ["/entrypoint.sh"]
CMD ["webserver"]